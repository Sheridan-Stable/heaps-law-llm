import os
import json

def parse_filename(filename):
    """
    Parse the filename to extract metadata.

    Expected filename format:
    Dataset_ModelName-ModelSize_PromptType_Type.json
    Example: PubMed_pythia-410m_fewshot_Close.json
    """
    base = os.path.basename(filename)
    name, ext = os.path.splitext(base)
    parts = name.split('_')

    if len(parts) < 4:
        print(f"Warning: Filename '{filename}' does not match the expected pattern. Skipping.")
        return None

    dataset = parts[0]
    model_part = parts[1]
    prompt_type = parts[2]
    type_part = parts[3]

    if '-' in model_part:
        model_name, model_size = model_part.split('-', 1)
    else:
        model_name = model_part
        model_size = "unknown"

    type_ = type_part.capitalize()  # Ensure consistency (e.g., 'close' -> 'Close')

    return {
        "dataset": dataset,
        "model_name": model_name,
        "model_size": model_size,
        "prompt_type": prompt_type,
        "type": type_
    }

def process(data):
    """
    Process the data to create newarray which contains cumulative total words
    and unique vocabulary size.

    Parameters:
    - data: List[List[str]] - A list where each item is a list of words.

    Returns:
    - newarray: List[List[int, int]] - Each sublist contains [total_words_so_far, vocab_size_so_far]
    """
    newarray = []
    overall_total_words = 0
    overall_unique_words = set()

    for idx, word_array in enumerate(data):
        if word_array is not None and isinstance(word_array, list):
            # Update total words
            overall_total_words += len(word_array)
            # Update unique vocabulary
            for word in word_array:
                if isinstance(word, str):
                    overall_unique_words.add(word)
                else:
                    print(f"Warning: Non-string word at index {idx}. Skipping this word.")

            # Append the current state to newarray
            newarray.append([overall_total_words, len(overall_unique_words)])
        else:
            print(f"Warning: Expected a list of words at index {idx}, but got {type(word_array)}. Skipping this item.")

    return newarray

def insert_into_nested_dict(nested_dict, keys, data):
    """
    Insert data into a nested dictionary based on the provided keys.

    Parameters:
    - nested_dict: dict - The dictionary to insert data into.
    - keys: list - A list of keys representing the path in the nested dictionary.
    - data: list - The newarray to insert at the deepest level.

    Example:
    insert_into_nested_dict(d, ['PubMed', 'pythia', '410m', 'fewshot', 'Close'], [[100, 50], [200, 80]])
    """
    current_level = nested_dict
    for key in keys[:-1]:
        if key not in current_level:
            current_level[key] = {}
        current_level = current_level[key]

    type_key = keys[-1]
    if type_key not in current_level:
        current_level[type_key] = []

    # Append the newarray to the list under the specific type
    current_level[type_key].append(data)

def main():
    """
    Main function to aggregate data from JSON files into a nested JSON structure.
    """
    aggregated_data = {}

    # Iterate through all files in the current directory
    for filename in os.listdir('.'):
        if not filename.endswith('.json'):
            continue  # Skip non-JSON files

        metadata = parse_filename(filename)
        if metadata is None:
            continue  # Skip files that don't match the pattern

        # Read JSON content
        try:
            with open(filename, 'r', encoding='utf-8') as f:
                data = json.load(f)  # Expecting a list of lists of words
        except json.JSONDecodeError as e:
            print(f"Error reading '{filename}': {e}")
            continue
        except Exception as e:
            print(f"Unexpected error reading '{filename}': {e}")
            continue

        # Ensure the JSON content is a list
        if not isinstance(data, list):
            print(f"Warning: The content of '{filename}' is not a list. Skipping.")
            continue

        # Process the data to create newarray
        newarray = process(data)

        # Prepare the path in the nested dictionary
        keys = [
            metadata["dataset"],
            metadata["model_name"],
            metadata["model_size"],
            metadata["prompt_type"],
            metadata["type"]
        ]

        # Insert the newarray into the nested dictionary
        insert_into_nested_dict(aggregated_data, keys, newarray)

    # Save the aggregated data to a JSON file
    output_filename = 'aggregated_output.json'
    try:
        with open(output_filename, 'w', encoding='utf-8') as f_out:
            json.dump(aggregated_data, f_out, indent=2)
        print(f"Aggregated data has been saved to '{output_filename}'.")
    except Exception as e:
        print(f"Error writing to '{output_filename}': {e}")

if __name__ == "__main__":
    main()

